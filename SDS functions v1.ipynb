{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample article\n",
    "WASHINGTON — The United States is sending to Ukraine up to $400 million in additional military equipment and supplies, including four more medium-range rocket systems and ammunition, as the embattled nation tries to repel Russia’s advances in the Donbas region.\n",
    "The four additional M142 High Mobility Artillery Rocket Systems, or HIMARS, will bring the total number sent to Ukraine to a dozen, a senior defense official told reporters in a briefing Friday. The official said the first eight HIMARS were particularly useful for Ukraine, as the fight in the Donbas has largely evolved into an artillery duel. The official refuted Russian reports that two of the delivered HIMARS were destroyed, and said all eight are accounted for and still in use by Ukraine.\n",
    "The military equipment being drawn down from U.S. stockpiles and sent to Ukraine also includes three tactical vehicles, demolition munitions, counter-battery systems and spare parts, among other equipment, so Ukraine can repair and maintain other systems that allies have sent in recent months.\n",
    "The shipment will also include 1,000 rounds of 155mm artillery ammunition, which the defense official described as a precision-guided type that would allow the Ukrainian military to better hit specific targets, which would save ammunition. The official would not confirm whether these shells will be the guided Excalibur artillery rounds, but said they have not been part of previous security assistance packages to Ukraine.\n",
    "HIMARS is a light, wheeled multiple rocket launcher, which Pentagon officials previously said was a “top priority” request by Ukraine. The U.S. undersecretary for defense for policy, Colin Kahl, told reporters last month that HIMARS allows Ukrainian forces to strike targets with greater range and precision than other artillery weapons that were sent.\n",
    "Ukrainian President Volodymyr Zelenskyy formally promised only to use HIMARS for defensive purposes and to avoid firing into Russian territory; this took place before the U.S. agreed to provide the systems in order to avoid escalating the conflict.\n",
    "The defense official said Russian claims HIMARS were used in strikes outside of Ukrainian territory are false, and that Russian forces, capabilities and logistics nodes within Ukraine are “absolutely fair targets.”\n",
    "The official said the weekslong process to train Ukrainian troops on how to use the high-end HIMARS platform has been a limiting factor, and is why they were delivered in batches of four at a time. The official said efforts to train more Ukrainians on HIMARS will continue, but would not say how many have so far been trained.\n",
    "The official said the HIMARS would arrive on the battlefield “rapidly,” but would not say how long their deployment might take.\n",
    "The official said Russian forces are making “very incremental, limited, hard-fought, highly costly progress” in some parts of Donbas, and that they are far behind their timelines and objectives. The official would not specify where Russian forces are believed to have been disrupted, but said they are behind the front lines in Donbas.\n",
    "Ukrainian forces are launching effective counteroffenses, the official said, and in the last week have started to use HIMARS strikes to seriously disrupt Russia’s ability to gain ground.\n",
    "“We don’t see this at all as Russia winning this battle,” the official said. “Certainly they’re not winning it relative to their initial objectives. They’ve been very much thwarted, but the fighting is hard.”\n",
    "The U.S. has been talking with allies and partners about other systems that could be sent to Ukraine, such as coastal defense capabilities, to move the nation away from Soviet legacy systems.\n",
    "While Ukraine has received a great deal of equipment from the U.S. and other partner nations, the official said, its military has been using it at such an intense pace that forces need resources to repair and sustain those systems.\n",
    "Providing this ability also sends Russia an important signal that Ukraine will be able to continue the fight, the official said.\n",
    "“If the Russians think they can outlast the Ukrainians, they need to rethink that,” the official said. “We are already pivoting towards thinking about what the Ukrainians will need in the months and years ahead.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/timothywee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('averaged_perceptron_tagger') #need this on first run\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "orig_stopWords = set(stopwords.words(\"english\"))\n",
    "stopWords_list = [',', '.', \"'s\", \"mr\", \"-\", '``', \"''\", \"said\", '$', 'month', 'months', 'year', 'years', 'date', 'dates', 'official', 'Ha']\n",
    "stopWords = orig_stopWords.union(stopWords_list)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summariser(text):\n",
    "\n",
    "    #tokenize text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    #Creating a dictionary of words and their frequencies\n",
    "    freqTable = dict()\n",
    "    for word in words:\n",
    "        word = word.lower() #convert to lowercase\n",
    "        if word in stopWords: #remove stopwords\n",
    "            continue\n",
    "        if word in freqTable:\n",
    "            freqTable[word] += 1\n",
    "        else:\n",
    "            freqTable[word] = 1\n",
    "\n",
    "    global sortedFreqTable #made it global because we will likely need it down the road\n",
    "    sortedFreqTable = sorted(freqTable.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    sentences = sent_tokenize(text) #split into sentences\n",
    "    global sentenceValue\n",
    "    sentenceValue = dict()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for word, freq in freqTable.items():\n",
    "            if word in sentence.lower(): #lowercase the sentence, so that there is no mismatching due to capitalization\n",
    "                if sentence in sentenceValue:\n",
    "                    sentenceValue[sentence] += freq #add the word's frequency to the sentence's frequency, to calculate the sentence's value. Higher value = more important words = more important sentence\n",
    "                else:\n",
    "                    sentenceValue[sentence] = freq\n",
    "\n",
    "    sumValues = 0\n",
    "    for sentence in sentenceValue:\n",
    "        sumValues += sentenceValue[sentence] #sum the values\n",
    "    \n",
    "    # Average value of a sentence from the original text\n",
    "    average = int(sumValues / len(sentenceValue))\n",
    "    \n",
    "    #adds the first sentence (usually the topic sentence in a news article) into the summary, followed by ones with high value scores. Optimised for news articles, not for general text.\n",
    "    global summary \n",
    "    summary = sentences[0]\n",
    "    for sentence in sentences[1:]:\n",
    "        if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.3 * average)):\n",
    "            summary += \" \" + sentence\n",
    "\n",
    "    return summary #, sortedFreqTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categoriser function\n",
    "#input: full article text \n",
    "#output: list of nouns and their frequencies, which can be used to categorise the article\n",
    "\n",
    "def categoriser(input_text):\n",
    "    # Replace 'US', 'U.S', with 'USA'. This is so that the POS tagger doesn't get confused by the word 'us' (collective term), and tags 'USA' as a noun.\n",
    "    # Because referring to the country is done in full caps, we can distinguish from 'us'\n",
    "    input_text = input_text.replace('US', 'USA')\n",
    "    input_text = input_text.replace('U.S', 'USA')\n",
    "\n",
    "    #replace 'United States' with 'USA'. This is to avoid the POS tagger tagging 'United' and 'States' as two different words.\n",
    "    input_text = input_text.replace('United States', 'USA')\n",
    "    \n",
    "    #lemmatize input_text \n",
    "    words = word_tokenize(input_text)\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(word))\n",
    "    input_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    #perform part-of-speech (POS) tagging on the input text, to identify nouns\n",
    "    pos_tagged = nltk.pos_tag(word_tokenize(input_text.lower()))\n",
    "\n",
    "    #convert nltk POS tags to wordnet POS tags\n",
    "    def pos_tagger(nltk_tag): #identifies nouns only\n",
    "        if nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        else:         \n",
    "            return None\n",
    "\n",
    "    #Mapping the pos_tagger function to the pos_tagged list. The pos_tagger function returns the wordnet.NOUN if the nltk_tag starts with 'N', otherwise it returns None.\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\n",
    "    #create array with only nouns\n",
    "    noun_list = []\n",
    "    for i in range(len(wordnet_tagged)):\n",
    "        if wordnet_tagged[i][1] == wordnet.NOUN:\n",
    "            noun_list.append(wordnet_tagged[i][0].capitalize()) #13 July 2022: capitalise category names\n",
    "\n",
    "    #make the terms 'HIMARS', 'UAV' and 'MANPAD' fully capitalised in noun_list. WIP: lemmatizer cannot catch plural forms of these terms, eg. MANPADs\n",
    "    noun_list = [x.replace('Himars', 'HIMARS') for x in noun_list]\n",
    "    noun_list = [x.replace('Uav', 'UAV') for x in noun_list]\n",
    "    noun_list = [x.replace('Uavs', 'UAV') for x in noun_list]\n",
    "    noun_list = [x.replace('Manpad', 'MANPAD') for x in noun_list]\n",
    "    noun_list = [x.replace('Manpads', 'MANPAD') for x in noun_list]\n",
    "\n",
    "    #sort noun_list by frequency\n",
    "    noun_freq = nltk.FreqDist(noun_list)\n",
    "    sorted_noun_freq = sorted(noun_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    #remove nouns in stopWords, with frequency < 2, that is not a word (like punctuation), and with length > 1 character (removes \"I\", etc.)\n",
    "    sorted_noun_freq = [x for x in sorted_noun_freq if x[0] not in stopWords and x[1] > 1 and x[0].isalpha() and len(x[0]) > 1]\n",
    "\n",
    "    return sorted_noun_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup 12 Jul; USE THE ONE ABOVE\n",
    "\n",
    "#categoriser function\n",
    "#input: full article text \n",
    "#output: list of nouns and their frequencies, which can be used to categorise the article\n",
    "\n",
    "def backup_categoriser(input_text):\n",
    "    tokenised_text = word_tokenize(input_text)\n",
    "    #lowercase tokenised_text \n",
    "    tokenised_text = [word.lower() for word in tokenised_text]\n",
    "\n",
    "    #remove 'has' from tokenised_text. Maybe do the pos tagging before tokenising.\n",
    "    tokenised_text = [word for word in tokenised_text if word != 'has']\n",
    "\n",
    "    #lemmatize tokenised_text\n",
    "    #tokenised_text = [lemmatizer.lemmatize(word) for word in tokenised_text]\n",
    "\n",
    "    pos_tagged = nltk.pos_tag(tokenised_text)\n",
    "\n",
    "    def pos_tagger(nltk_tag): #identifies nouns only\n",
    "        # if nltk_tag.startswith('J'):\n",
    "        #     return wordnet.ADJ\n",
    "        # elif nltk_tag.startswith('V'):\n",
    "        #     return wordnet.VERB\n",
    "        if nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        # elif nltk_tag.startswith('R'):\n",
    "        #     return wordnet.ADV\n",
    "        else:         \n",
    "            return None\n",
    "\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\n",
    "    #create array with only nouns\n",
    "    noun_list = []\n",
    "    for i in range(len(wordnet_tagged)):\n",
    "        if wordnet_tagged[i][1] == wordnet.NOUN:\n",
    "            noun_list.append(wordnet_tagged[i][0])\n",
    "\n",
    "    #sort noun_list by frequency\n",
    "    noun_freq = nltk.FreqDist(noun_list)\n",
    "    sorted_noun_freq = sorted(noun_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    #remove nouns with frequency 1\n",
    "    sorted_noun_freq = [x for x in sorted_noun_freq if x[1] > 1 and x[0].isalpha() and len(x[0]) > 1]\n",
    "\n",
    "    #remove nouns in StopWords\n",
    "    sorted_noun_freq = [x for x in sorted_noun_freq if x[0] not in stopWords] \n",
    "\n",
    "    return sorted_noun_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONDON/FRANKFURT (REUTERS) - The biggest single pipeline carrying Russian gas to Germany started annual maintenance on Monday (July 11), with flows expected to stop for 10 days, but governments, markets and companies are worried the shutdown might be extended due to war in Ukraine. The Nord Stream 1 pipeline transports 55 billion cubic metres (bcm) a year of gas from Russia to Germany under the Baltic Sea. Europe fears Russia may extend the scheduled maintenance to restrict European gas supply further, throwing plans to fill storage for winter into disarray and heightening a gas crisis that has prompted emergency measures from governments and painfully high bills for consumers. German Economy Minister Robert Habeck has said the country should confront the possibility that Russia will suspend gas flows through Nord Stream 1 beyond the scheduled maintenance period. There are other big pipelines from Russia to Europe but flows have been gradually declining, especially after Ukraine halted one gas transit route in May, blaming interference by occupying Russian forces. \"The abrupt end of Russian gas imports would also have a significant impact on the workforce in Germany ... Around 5.6 million jobs would be affected by the consequences,\" vwb's managing director Bertram Brossardt said. In an interview with Reuters last Thursday, he said the Dutch Groningen gas field could still be called upon the help neighbouring countries in the event of a complete cut off in Russian supplies, but ramping up production would risk causing earthquakes. Extended maintenance could also result in more Russian gas production shut-ins, relative to the 9 per cent year-to-date year-on-year decline in Gazprom production reported so far, Goldman Sachs said.\n",
      "[('Gas', 21), ('Maintenance', 9), ('Russia', 9), ('Pipeline', 8), ('Stream', 7), ('Energy', 6), ('Flow', 5)]\n"
     ]
    }
   ],
   "source": [
    "original_text = input(\"input text here:\")\n",
    "\n",
    "print(summariser(original_text))\n",
    "print(categoriser(original_text)[:7]) #print first 7 entries in categoriser; can print more\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "nltk.download('punkt') # first-time use only\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "d1 = input(\"input D1 here:\")\n",
    "d2 = input(\"input D2 here:\")\n",
    "# d3 = input(\"input D3 here:\") #to input more documents\n",
    "# d4 = input(\"input D4 here:\") #to input more documents\n",
    "documents = [d1, d2] #d3, d4]\n",
    "\n",
    "nltk.download('wordnet') # first-time use only\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "\n",
    "def cos_similarity(textlist):\n",
    "    tfidf = TfidfVec.fit_transform(textlist)\n",
    "    return (tfidf * tfidf.T).toarray()\n",
    "\n",
    "cos_similarity(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "#Initialise BERT ZeroShot model\n",
    "checkpoint = \"facebook/bart-large-mnli\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "classifier = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artillery 1.0\n",
      "Missile 0.27611\n",
      "Tank 0.23053\n",
      "Infrastructure 0.19795\n",
      "MANPAD 0.1238\n"
     ]
    }
   ],
   "source": [
    "original_text = input(\"input text here:\")\n",
    "\n",
    "\n",
    "#BERT ZeroShot categorizer \n",
    "#input: full article text\n",
    "#output: list of categories and their confidence scores \n",
    "\n",
    "\n",
    "labels = ['Tank', 'Artillery', 'UAV', 'Fighter Aircraft', 'Helicopter', 'Missile', 'MANPAD', 'Infrastructure'] #edit candidate labels here\n",
    "\n",
    "results = classifier(original_text, candidate_labels=labels)\n",
    "\n",
    "print(results['labels'][0], round(results['scores'][0]/results['scores'][0], 5))\n",
    "print(results['labels'][1], round(results['scores'][1]/results['scores'][0], 5))\n",
    "print(results['labels'][2], round(results['scores'][2]/results['scores'][0], 5))\n",
    "print(results['labels'][3], round(results['scores'][3]/results['scores'][0], 5))\n",
    "print(results['labels'][4], round(results['scores'][4]/results['scores'][0], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helicopter 0.18488530814647675\n"
     ]
    }
   ],
   "source": [
    "#print top category and confidence score\n",
    "print(results['labels'][0], results['scores'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae0bcfd361d2c039dac95f781bb94ff6597fd331577330b34ee3756962a31d06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
